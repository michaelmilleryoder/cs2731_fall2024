[
  {
    "objectID": "hw2.html",
    "href": "hw2.html",
    "title": "Homework 2: Text classification",
    "section": "",
    "text": "Due 2024-10-03, 11:59pm. Instructions last updated 2024-09-30."
  },
  {
    "objectID": "hw2.html#learning-objectives",
    "href": "hw2.html#learning-objectives",
    "title": "Homework 2: Text classification",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter completing this assignment, students will be able to:\n\nDemonstrate how weights adjust to better fit training input in stochastic gradient descent\nImplement a text classification system using both feature-based and neural network approaches\nIdentify informative features in a feature-based text classification system\nAnalyze errors in an NLP system"
  },
  {
    "objectID": "hw2.html#part-1-learning-weights-in-logistic-regression",
    "href": "hw2.html#part-1-learning-weights-in-logistic-regression",
    "title": "Homework 2: Text classification",
    "section": "Part 1: Learning weights in logistic regression",
    "text": "Part 1: Learning weights in logistic regression\nYou are training a classifier for reviews of a new product recently released by a company. You design a couple of features, \\(x_1\\) and \\(x_2\\). You will be using logistic regression. With an initialization of the weights \\(w_1\\), \\(w_2\\) and \\(b\\) (the bias) all set = 0 and a learning rate \\(\\eta=0.2\\), calculate the weights after processing each of the following 3 inputs:\n\n\\[x_1 = 2, x_2 = 1, y = 1\\]\n\\[x_1 = 1, x_2 = 3, y = 0\\]\n\\[x_1 = 0, x_2 = 4, y = 0\\]\n\nDuring calculations, keep at least 3 significant digits for values. Points will not be taken off for slight differences due to rounding.\n\nDeliverables for Part 1\nIn the report:\n\nGive weights after training on each data point (3 total weight changes, one after each timestep/data point).\nShow your work in calculating the values of the weights after training on each data point.\nBriefly comment on any shift in weights from positive to negative or negative to positive and why this was the case."
  },
  {
    "objectID": "hw2.html#part-2-implement-a-deception-classifier",
    "href": "hw2.html#part-2-implement-a-deception-classifier",
    "title": "Homework 2: Text classification",
    "section": "Part 2: Implement a deception classifier",
    "text": "Part 2: Implement a deception classifier\nIn this portion, you will design and implement a program to classify if a comment from a player of the Diplomacy game is truthful or not. You can use any packages you want for this (scikit-learn, spaCy, NLTK, Gensim, code from Homework 1, etc). Any packages used should be specified in the README.txt file, along with version numbers for Python and all packages. If you will be using a language other than Python, please let us know before submitting. Your script should be able to take the filename of a dataset as a single keyword argument."
  },
  {
    "objectID": "hw2.html#dataset",
    "href": "hw2.html#dataset",
    "title": "Homework 2: Text classification",
    "section": "Dataset",
    "text": "Dataset\nHere is the dataset that you should download for this assignment:\n\ndiplomacy_cv.csv. This dataset has a variety of fields, but the most important are:\n\ntext: the text of the comment\nintent: 0 for truth, 1 for lie\n\ndiplomacy_test.csv (only necessary if participating in the optional challenge). This data has the same fields as the training data. You will use this in the optional challenge competition hosted on Kaggle.\n\nThis dataset is from a recording of online players of Diplomacy, as presented in Peskov et al. 2020. Negotiation and back-stabbing are key elements of the Diplomacy game."
  },
  {
    "objectID": "hw2.html#feature-based-logistic-regression-models",
    "href": "hw2.html#feature-based-logistic-regression-models",
    "title": "Homework 2: Text classification",
    "section": "2.1 Feature-based logistic regression models",
    "text": "2.1 Feature-based logistic regression models\nIn this section, you will build a logistic regression model based on bag-of-word features and/or features of your own design. You can do whatever preprocessing you see fit. You will report performance using 5-fold cross-validation on the diplomacy_cv.csv dataset, which you will set up. Make sure to just extract features (bag-of-words, etc) from the training set and not the test folds within cross-validation.\n\nTasks for section 2.1\nImplement and try the following feature and model combinations:\n\nLogistic regression with bag-of-words (unigram) features. Build a logistic regression classifier that uses bag-of-words (unigram) features.\nLogistic regression with your own features/change in preprocessing. Design and test at least two modifications (custom features or preprocessing changes) to unweighted unigram features. Note that these features can be used in conjunction with bag-of-words features or by themselves. Possible features/changes to add and test include:\n\nTf-idf transformed bag-of-words features\nChanging count bag-of-words features to binary 0 or 1 for the presence of unigrams\nN-gram features (sequences of words) beyond the single words used for the bag-of-words features\nDifferent preprocessing (stemming, different tokenizations, stopword removal)\nReducing noisy features with feature selection\nCounts or added weight from custom word lists\nStatic word embeddings of your choice (do not use any contextualized word embeddings, such as BERT, for this part)\nAny other custom-designed feature (such as length of input, number of capitalized words, etc)\n\n\nYou will thus have 3 total logistic regression models: one using bag-of-word features and 2 with your own selected features or preprocessing changes.\nIn the report, please provide:\n\nA table of 5-fold cross-validation performance scores for models trained on each set of features. Include accuracy as well as precision, recall, and f1-score for the positive (lying) class. Please average these scores across the 5 folds for each evaluation metric (there is no need to include scores for each fold).\nFor each feature or change in input text processing:\n\nDescribe your motivation for including the feature\nDiscussion of results: Did it improve performance or not? (Either result is fine. It is not necessary to beat logistic regression with unigram features. This is a very difficult task.)\n\nFor a feature-based model of your choice (not a neural model):\n\nExtract and discuss the most informative features that are mostly strongly positively and negatively associated with deception. Report the 5 features with the highest weights and 5 features with the lowest (negative) weights. Discuss how these may or may not make sense for this task. You may adapt code provided by the instructor in the Naive Bayes example (notebook here), use another source online, or write your own. Give specific informative features, such as particular words (e.g. “actually”) for bag-of-words features, instead of sets of features like “tf-idf unigram features”.\nDo an error analysis. Provide a confusion matrix, sample examples from both false negatives and false positives and present a few of them in the report. Do you see any patterns in these errors? How might these errors be addressed with different features or if the system could understand something else? (You don’t have to implement these, just speculate.)"
  },
  {
    "objectID": "hw2.html#neural-network-based-approaches",
    "href": "hw2.html#neural-network-based-approaches",
    "title": "Homework 2: Text classification",
    "section": "2.2 Neural network-based approaches",
    "text": "2.2 Neural network-based approaches\nIn this section, you will build and evaluate neural network-based classifier on deception classification. For example, you could implement a feedforward neural network that uses pre-trained static word embeddings (word2vec, GloVe, FastText, etc) as input. You can download these pre-trained word embeddings from wherever you like (you don’t have to train your own). To represent the document, you could take the average word embeddings of the input sentence or choose another function. You can choose which activation function to use and other hyperparameters. You are also welcome to try other methods we haven’t yet covered in class, such as LSTMs, convolutional neural networks, BERT, or other LLMs. As long as the technique uses neural networks at some point in its architecture and involves some sort of training or fine-tuning of a model, it will be accepted. Simply prompting a pre-trained LLM to classify the instances (“zero-shot” or “in-context” learning) will not be sufficient. You can also incorporate any non-text metadata features in this part. If you have questions about what is acceptable, ask the instructor or TA.\nYou will again use 5-fold cross validation on the dataset. There is no need for this model to outperform the logistic regression model you made.\n\nTasks for section 2.2\nImplement a neural network-based deception classifier, such as a feedforward neural network with static word embeddings as input.\nIn the report, please provide:\n\nPerformance scores for this model. Include accuracy as well as precision, recall, and f1-score for the positive (polite) class. This can be an additional row in the table with other performance scores. It does not need to outperform the logistic regression model.\nDiscuss the motivation for any choices you made as far as neural classifier, word embedding types, pretraining dataset, and/or how you represented the document, or if you experimented with multiple of these options.\nDiscuss the motivation for any choices you made as far as network architecture (number and dimensions of hidden layers) or hyperparameters (learning rate, number of epochs, etc). Note if you experimented with any of these options."
  },
  {
    "objectID": "hw2.html#optional-submit-your-classifier-in-the-class-challenge",
    "href": "hw2.html#optional-submit-your-classifier-in-the-class-challenge",
    "title": "Homework 2: Text classification",
    "section": "2.3 (Optional) Submit your classifier in the class challenge",
    "text": "2.3 (Optional) Submit your classifier in the class challenge\nOptionally, you can submit your classifier to run on a hidden held-out test set as part of a class competition. Bonus points will be awarded in the competition as follows, as measured by accuracy on our held-out test set.\n\n4 bonus points for the best-performing logistic regression classifier\n4 bonus points for the best neural network classifier\n2 bonus points for the 2nd best-performing logistic regression classifier\n2 bonus points for the 2nd best-performing neural network classifier\n1 bonus point for submitting any system (either logistic regression or neural network)\n\n\nHow to submit your classifier\nThere are two Kaggle competitions. See these pages for instructions on how to submit:\n\nLogistic regression Kaggle competition\nNeural network Kaggle competition\n\nYou will need to create a Kaggle account to submit. Please provide your Kaggle username used in the competition in your report so we can assign any bonus points. Note that this username will be visible in a leaderboard to other challenge competition participants."
  },
  {
    "objectID": "hw2.html#notes",
    "href": "hw2.html#notes",
    "title": "Homework 2: Text classification",
    "section": "Notes",
    "text": "Notes\n\nDon’t feel like you need to write things from scratch; use as many packages as you want. Google and Stack Overflow and NLP/ML software documentation are your friend! Adapting and consulting other approaches is fine and should be noted in comments in the code and/or in the README.txt. Just don’t use complete, fully-formed implementations for this (including from generative AI tools). Use all resources as aids, not as a final product.\nOptionally, you may incorporate any form of regularization that you like\nThis homework is designed to be able to be run on a laptop with CPUs, not GPUs. Let the instructor/TA know if you are having difficulty completing it with the resources you have."
  },
  {
    "objectID": "hw2.html#deliverables",
    "href": "hw2.html#deliverables",
    "title": "Homework 2: Text classification",
    "section": "Deliverables",
    "text": "Deliverables\n\nYour report with results and answers to questions in Part 1 and Part 2, named hw2_{your pitt email id}.pdf. No need to include @pitt.edu, just use the email ID before that part. For example: report_mmyoder_hw2.pdf.\n\nIf participating in the challenge, the Kaggle username you used to submit your predictions\nIf participating in the challenge, your code used for that in a file named hw2_{your pitt email id}_test.py.\n\nYour code used to train models and estimate performance with cross-validation in a file named hw2_{your pitt email id}_cv.py.\nA README.txt file explaining\n\nhow to run the code you used to train your models and estimate cross-validation performance\nthe computing environment you used, including the names and versions of programming languages and packages used, in case we replicate your experiments. A requirements.txt file for setting up the environment is useful if there are many packages.\nany additional files needed to run the code, such as the names and versions of pretrained embeddings\nany additional resources, references, or web pages you’ve consulted\nany person with whom you’ve discussed the assignment and describe the nature of your discussions\nany generative AI tool used, and how it was used\nany unresolved issues or problems\n\n\nPlease submit all of this material on Canvas. Do not zip all files. We will grade your report and look over your code."
  },
  {
    "objectID": "hw2.html#grading",
    "href": "hw2.html#grading",
    "title": "Homework 2: Text classification",
    "section": "Grading",
    "text": "Grading\nSee rubric on Canvas."
  },
  {
    "objectID": "hw2.html#acknowledgments",
    "href": "hw2.html#acknowledgments",
    "title": "Homework 2: Text classification",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis assignment is inspired from a homework assignment by Prof. Diane Litman. Data is from Peskov et al. 2020."
  },
  {
    "objectID": "hw1.html",
    "href": "hw1.html",
    "title": "Homework 1: Vector space word similarity",
    "section": "",
    "text": "Due 2024-09-19, 11:59pm. Instructions last updated 2024-09-09.\nIn this assignment, you’ll build representations for documents and words based on the bag-of-words model. You’ll implement 2 popular weighting schemes for these vectors: tf-idf and PPMI, both discussed in Chapter 6 of the textbook. Then you’ll compare these weighting schemes on learning word similarity and apply one of them, PPMI, to examine social bias in an NLP corpus."
  },
  {
    "objectID": "hw1.html#learning-objectives",
    "href": "hw1.html#learning-objectives",
    "title": "Homework 1: Vector space word similarity",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter completing this assignments, you will be able to:\n\nLoad in text data and manipulate it in Python\nDemonstrate that a model’s notions of word similarity comes from the contexts of neighboring words in corpora\nImplement vectors for words, the predominant representation of semantics in NLP\nInvestigate how NLP corpora can potentially encode harmful social biases through word associations"
  },
  {
    "objectID": "hw1.html#datasets-and-skeleton-code",
    "href": "hw1.html#datasets-and-skeleton-code",
    "title": "Homework 1: Vector space word similarity",
    "section": "Datasets and skeleton code",
    "text": "Datasets and skeleton code\nHere are the materials to download for use in this assignment:\n\nSkeleton Python code. You will need to have a Python environment with scipy and numpy packages. Some functions are stubs in the python code. You will need to fill them out.\nCSV of the complete works of Shakespeare\nVocab of the complete works of Shakespeare\nList of all plays in the dataset\nList of identity labels from Rudinger et al. 2017\nSNLI corpus\n\nThis corpus is selections from SNLI, a corpus used for the NLP task of “natural language inference” (see Bowman et al. 2015 dataset paper). Each line contains a sentence that is either a “premise” (an image caption) or a “hypothesis” produced by annotators to be in a certain logical relation with the associated premise (entailment, neutral, contradiction). You don’t need to worry about these details, but the sentenceID column is a unique index for each sentence and captionID is an ID for all sentences associated with same caption/premise."
  },
  {
    "objectID": "hw1.html#part-1-vector-spaces",
    "href": "hw1.html#part-1-vector-spaces",
    "title": "Homework 1: Vector space word similarity",
    "section": "Part 1: Vector spaces",
    "text": "Part 1: Vector spaces\n\nImplementation\nOpen up the skeleton.py file. Complete the function stubs using the information below.\n\nTerm-document matrix\nWrite code to compile a term-document matrix for Shakespeare’s plays in the function stub create_term_document_matrix. Follow the description in the textbook:\n\n\nIn a term-document matrix, each row represents a word in the vocabulary and each column represents a document from some collection. The figure below shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times a particular word (defined by the row) occurs in a particular document (defined by the column). Thus clown appeared 117 times in Twelfth Night\n\n\nHere is an example of a term-document matrix (you will build a different one in the homework):\n\n\n\n\n \n\n\nAs You Like It\n\n\nTwelfth Night\n\n\nJulius Caesar\n\n\nHenry V\n\n\n\n\n\n\nbattle\n\n\n1\n\n\n1\n\n\n8\n\n\n15\n\n\n\n\nsoldier\n\n\n2\n\n\n2\n\n\n12\n\n\n36\n\n\n\n\nfool\n\n\n37\n\n\n58\n\n\n1\n\n\n5\n\n\n\n\nclown\n\n\n5\n\n\n117\n\n\n0\n\n\n0\n\n\n\n\nThe dimensions of your term-document matrix will be the number of documents \\(D\\) (in this case, the number of Shakespeare’s plays that we give you in the corpus) by the number of unique word types \\(\\vert V \\vert\\) in that collection. The columns represent the documents, and the rows represent the words, and each cell represents the frequency of that word in that document.\nWrite this code from scratch. You can use packages like NumPy as helper packages, but do not use packages that directly do this computation for you (like scikit-learn or gensim).\n\n\nTerm-Context Matrix\nInstead of using a term-document matrix, a more common way of computing word similarity is by constructing a term-context matrix (also called a term-term or word-word matrix), where columns are labeled by words rather than documents. The dimensionality of this kind of a matrix is \\(\\vert V \\vert\\) by \\(\\vert V \\vert\\). Each cell represents how often the word in the row (the target word) co-occurs with the word in the column (the context) in a training corpus. You get to decide what are considered units that form boundaries of that context. For example, do two words have to co-occur within a close window in the same line of the play, or just in the same play? You also can decide when it makes sense for a word to co-occur with itself in the term-context matrix. That is, will the cell for when the same word is target and context always stay 0? Note that there may be vectors where all elements are 0 if a word only appears in documents where it is the only word in the document.\nSpecifically, implement the create_term_context_matrix function. This function specifies the size of the word window around the target word that you will use to gather its contexts. For instance, if you set that variable to be 4, then you will use 4 words to the left of the target word, and 4 words to its right for the context. In this case, the cell represents the number of times in Shakespeare’s plays the column word occurs in +/-4 word window around the row word.\nWrite this code from scratch, i.e. do not use additional packages that directly compute these matrices.\n\n\nEvaluating vector spaces\nSo far we have created 2 vector spaces for the words in Shakespeare, one with a dimension of \\(D\\) and another of dimension \\(\\vert V \\vert\\). Now we will try to evaluate how good our vector spaces are. We can do this with an intrinsic evaluation approach by seeing what words within the vocab are most similar to each other/are synonyms with each other and assessing if the output is reasonable.\nImplement the rank_words function, which will take a target word index and return a list sorted from most similar to least similar using the cosine similarity metric. For the purposes of the assignment, let’s just look at the top 10 words that are most similar to a target word between both the term-document matrix and the term-context matrix (with a window size of your choice). Are those 10 words good synonyms? The skeleton code provides an example of using rank_words and looking at similar words using the word ‘juliet’. It is okay if the top ranked word is the word itself, with a cosine similarity of 1.\nWrite this code from scratch, i.e. do not use additional packages that directly compute these matrices.\n\n\nWeighting terms with tf-idf and PPMI\nYour term-context matrix contains the raw frequency of the co-occurrence of two words in each cell and your term-document matrix contains the raw frequency of words in each of the documents. Raw frequency turns out not to be the best way of measuring the association between words. There are several methods for weighting words so that we get better results.\nTake your term-document matrix and implement the weighting schemes Term frequency inverse document frequency (tf-idf) and Positive pointwise mutual information which are defined in Sections 6.5-6.6 of the textbook. These are the function stubs create_tf_idf_matrix and create_ppmi_matrix.\n\n\n\nQuestions for the report\n1.1. In our term-document matrix, the rows are word vectors of \\(D\\) dimensions. Do you think that’s enough to represent the meaning of words? Why or why not?\n1.2. Provide the top 10 associated words and cosine similarities (the output from rank_words) with juliet and at least 2 other target words of your choice for both term-document and term-context vector spaces.\n1.3. Just considering the term-document and term-context matrices without any tf-idf or PPMI weighting, which do you think produces similar words that make more sense than others? Why do you think that is the case? Back up your conclusions by referring to the top associated term lists you provided.\n1.4. Explain any decisions you made in implementing your functions, such as whether you allowed a target word to co-occur with itself as a context word, and which window size you chose for the term-context matrix. How might any decisions you make impact our results now?\n1.5. Provide the top 10 associated words (the output from rank_words) with juliet and at least 2 other target words of your choice for tf-idf-weighted term-document matrices and PPMI-weighted term-context matrices.\n1.6. Compare the ranked word similarities between weighting with tf-idf and using the unweighted term-document matrix. Which do you think produces similar words that make more sense? Back up your conclusions with specific examples.\n1.7. Compare the ranked word similarities between weighting with PPMI and using an unweighted term-context matrix. Which do you think produces similar words that make more sense? Back up your conclusions with specific examples.\n1.8. Overall, do some approaches appear to work better than others, i.e produce better synonyms? Do any interesting patterns emerge? Discuss and point to specific examples."
  },
  {
    "objectID": "hw1.html#part-2",
    "href": "hw1.html#part-2",
    "title": "Homework 1: Vector space word similarity",
    "section": "Part 2",
    "text": "Part 2\nIn this part, you will measure associations between words in a commonly used NLP corpus, SNLI, and comment on the potential for encoding problematic social biases.\n\nImplementation\nThere is no skeleton code for this section, but you can reuse code from Part 1.\nFirst, write a loader for text from the SNLI corpus into a similar format as the Shakespeare corpus was loaded. You can use the sentenceID column as the document name, though this will not be as important since you’ll only be building a term-context matrix. You’ll still want to tokenize and lowercase the input as was done with the Shakespeare corpus. It is also encouraged to remove stopwords (such as with a list from the NLTK package) and remove low-frequency terms from the vocabulary.\nThen, build a term-context matrix in a similar fashion as with the Shakespeare corpus and apply PPMI weighting. You can choose the size of the context window. You can also use the whole sentence as the context. If this matrix is too big or taking too long to calculate, filter to just words that occur over some frequency threshold in the entire corpus.\n\nFind words associated with identity labels in SNLI\nNow you will use the list of identity terms provided above in the ‘Datasets and skeleton code’ section. Examine which words are highly associated with selected identity labels in the SNLI corpus with PPMI using the rank_words function. Choose identity labels that are related, such as multiple terms for gender, multiple terms for race/ethnicity, or other relations. Look for any associations that may reflect social stereotypes or possible representational harms in machine learning, defined below from Blodgett et al. 2020:\n\nRepresentational harms arise when a system (e.g., a search engine) represents some social groups in a less favorable light than others, demeans them, or fails to recognize their existence altogether.\n\nTypes of representational harms from Blodgett et al. 2020 include:\n\n\nStereotyping that propagates negative generalizations about particular social groups\nDifferences in system performance for different social groups, language that misrepresents the distribution of different social groups in the population, or language that is denigrating to particular social groups.\n\n\n\n\nQualitative analysis of word contexts\nNow you will explore the contexts in the dataset that lead to high PMI association with context words, especially for any words that show social bias (if you found any). 1st-order similarity is when a target word (in this, case, an identity label) occurs in the same document with a top-associated term. This might not be very informative to see how these words are related in the dataset. If not, look at 2nd-order similarity, in which the two words occur with similar context words. This can also be examined by looking at the target vectors for the identity term and the highly associated other term in the term-context matrix. These vectors may share high values in dimensions that correspond to certain context words.\n\n\n\nQuestions for the report\n2.1 Provide the top 10 associated context words (by PPMI, in the SNLI corpus) for at least 4 identity labels of your choice. Choose identity labels that are related, such as multiple terms for gender, multiple terms for race/ethnicity, or other relations.\n2.2. Do you see any associations learned by this bag-of-words model on the SNLI corpus that be representational harms, such as negative social stereotypes? Compare the top PPMI words for certain identity terms with other related ones (such as men compared with women). Discuss and provide selected results. If you don’t find any representational harms (that’s okay), provide examples of what you examined and how you interpreted those associations. If you do find problematic associations, specify how they could be harmful.\n2.3. For at least 4 pairs of identity terms and highly associated words, provide the document contexts in the SNLI dataset that contribute to this association. Provide actual sentences from the SNLI corpus where either:\n\nan identity term occurs together with the associated word you found (1st-order similarity) or\nan identity term occurs separately from the associated word, but occurs with similar context words (2nd-order similarity). You’ll want to compare values of dimensions in the vectors for both words in this case.\n\nDiscuss any findings, particularly related to any associations you found to represent harmful stereotypes."
  },
  {
    "objectID": "hw1.html#deliverables",
    "href": "hw1.html#deliverables",
    "title": "Homework 1: Vector space word similarity",
    "section": "Deliverables",
    "text": "Deliverables\n\nYour report with results and answers to questions in Part 1 and Part 2, named report_{your pitt email id}_hw1.pdf. No need to include @pitt.edu, just use the email ID before that part. For example: report_mmyoder_hw1.pdf.\nYour implementations for the functions in the skeleton code hw1_skeleton_{your pitt email id}.py. You are welcome to put code for Part 2 in the same or a different file. If it’s different, please where it is in the README.txt.\n\nYou are welcome to import any packages you need but please don’t modify the function that has already been implemented.\n\nA README.txt file explaining\n\nhow to run your code\nthe computing environment you used; what programming language you used and the major and minor version of that language; what packages did you use in case we replicate your experiments (a requirements.txt file for setting up the environment may be useful if there are many packages).\nany additional resources, references, or web pages you’ve consulted\nany person with whom you’ve discussed the assignment and describe the nature of your discussions\nany generative AI tool used, and how it was used\nany unresolved issues or problems\n\n\nPlease submit all of this material on Canvas as individual files. Do not submit a zip file. Only files with .pdf, .txt, or .py file extensions will be accepted. If you used Jupyter Notebook to complete the assignment, please download it as a .py script. We will grade your report and look over your code."
  },
  {
    "objectID": "hw1.html#grading",
    "href": "hw1.html#grading",
    "title": "Homework 1: Vector space word similarity",
    "section": "Grading",
    "text": "Grading\nSee rubric on Canvas. This assignment is worth 56 points."
  },
  {
    "objectID": "hw1.html#acknowledgments",
    "href": "hw1.html#acknowledgments",
    "title": "Homework 1: Vector space word similarity",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis assignment is adapted from Prof. Diane Litman and Prof. Mark Yatskar, as well from Rudinger et al. 2017."
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project",
    "section": "",
    "text": "Last revised 2024-12-03.\nA major component of this course is a hands-on final project guided by students’ own interests. In this project, students will demonstrate an ability to summarize current approaches and challenges in a subfield of NLP and implement some sort of contribution (however small) to this NLP area of research or practice."
  },
  {
    "objectID": "project.html#groups",
    "href": "project.html#groups",
    "title": "Project",
    "section": "Groups",
    "text": "Groups\nProjects will be done in groups of 2-4 students. Groups will be assigned by teaching staff based on interests, skills, and group preferences from students."
  },
  {
    "objectID": "project.html#deliverables",
    "href": "project.html#deliverables",
    "title": "Project",
    "section": "Deliverables",
    "text": "Deliverables\n\nProject idea submission form. Due 09-20. With this form, you can fill out potential project ideas you might be interested in working on. You can fill out ideas from the example projects listed on this website, ideas you have from research you are a part of, interesting text datasets you’d like to work on, really anything! You can fill out as many ideas as you’d like in this form. Ideas do not have to be fully sketched out. Submitting an idea does mean you will necessarily work on it. These ideas will be presented to all students anonymously. Each student must submit at least one idea for credit on this assignment, even if it’s just chosen from the example projects.\nProject idea ranking and survey. Due 09-26. In a form, students will rank which project ideas they would prefer to work on, as well as list any personnel preferences, interests and skills you have. Teaching staff will take all of this information into account when assigning groups.\nProject proposal presentation. In class 10-16. Groups will make a brief presentation to the class outlining their proposed project, with Q&A and opportunities for feedback from other students. Please plan for maximum 7-minute presentations not including Q&A, which will be held right afterward for each group. Please add your slides to this shared PowerPoint presentation. Presentations are not graded. Cover at least these key points:\n\nProject motivation (what is the value of this work?)\nBriefly, what 1-2 other related papers have done\nWhat data you are planning to use\nWhat approach/methods will you be taking\nEvaluation of your approach (or dataset, if it’s a dataset contribution)\n\nProject proposal and literature review. Due 10-18. Please submit one per group on Canvas. There is no required length or format for this report, but it is recommended to use the ACL format that the final report will be formatted in. This proposal will be a report with answers to the following questions:\n\nWill you be making a contribution of a new dataset, new application, new approach, combination of these or some other type of contribution?\nWhat is the problem or task you are focusing on?\nWhat is the expected output of your project? This could be a new approach and its evaluation on particular datasets. It could be a new dataset with a particular format, or new analysis. In any of these cases, please describe the format of your desired output.\nHow does your contribution build on or extend prior work? This literature review will be of at least 3 papers relevant to your project area. It will group and summarize relevant papers into types of tasks, datasets, and/or approaches. Good places to look for NLP papers include the ACL Anthology, Semantic Scholar, and Google Scholar.\nWhat data are you using (or contributing)? Please explain where these datasets are from and how they were constructed.\nWhat algorithm or approach are you taking to address the task?\nHow are you evaluating your contribution? What performance metrics are you going to use?\nWhat kinds of ethical issues may be raised by your model or data?\nWhat are the proposed steps needed for completion of the project?\nWhat are roles and tasks of each person in the group? Though group members will contribute in various capacities, it is best if each person is responsible for at least one aspect of the project.\n\nProject peer review. Due 11-14. In a form, you will rate your own performance and the performance of other group members. This feedback will not be used for grading, but to identify any workload distribution issues early on and assign roles accordingly.\nProgress report. Due 11-14. A brief (max 3 pages) progress report of a basic working system. Not everything needs to be done or fully functional, but there needs to be some sort of basic functionality. Also list any questions you have or resources you will need to successfully complete the project by the final deadline, if you have any. This report should be in the ACL format that the final report will be in and should maximum 3 pages, not including references. You do not have to repeat information from the project proposal except for basic descriptions of the project.\nFinal presentation. In class on 12-11. Groups will present their finished work to the group, with Q&A and feedback opportunities from students. Please prepare a maximum 5-minute presentation in which you can divide up speaking responsibilities however you see fit, though having more than one group member is encouraged. Add your slides to a shared PowerPoint (to be uploaded soon). Cover at least these key points:\n\nProject motivation (briefly)\nData\nMethods, or annotation/collection approach for dataset projects\nResults\n\nFinal report. Due 12-12. At the end of the course, groups will provide a written report of their project. This report will be in the ACL format found here (Overleaf template here. The report should be a maximum of 8 pages, not including limitations, ethics, group member task breakdown, references sections or appendices. Outstanding reports would be of a quality and structure that could be submitted to an NLP workshop or conference, but other types of projects can also achieve an A. There is flexibility in section names, but please provide information about the following aspects of the project:\n\nProject motivation\nLiterature review. Please provide full citations in a references sections for works cited throughout the paper (not just URLs).\nData\nMethods. Please clearly specify which techniques are novel/your own versus methods directly or indirectly from prior work (which is also fine).\nResults\nDiscussion\nFuture work. This is a good place to describe things you thought about but never had time to complete!\nLimitations (doesn’t count toward page limit)\nEthical issues (doesn’t count toward page limit)\nGroup member task breakdown (doesn’t count toward page limit). This section details the high-level tasks that each group member completed.\nReferences (doesn’t count toward page limit)\nAppendices (optional, doesn’t count toward page limit). Additional figures or explanation in one or more appendices is allowed, but they will not necessarily be considered in grading.\n\n\nHere is the rubric that will be used in grading:\n\n\n\n\n\n\n\nRubric category\nPoints\n\n\n\n\nClear motivation for the work is provided\n4\n\n\nResearch questions and/or task definition is clear\n8\n\n\nSufficient grounding in relevant related literature\n10\n\n\nApplicable dataset/s are chosen\n5\n\n\nMethods are relevant.For new approach contributions, multiple methods are compared.For dataset contributions, annotation methodology is explained\n15\n\n\nResults are provided.For new approach contributions, results from multiple methods (at least one baseline) are presented.For dataset contributions, this may be a single set of results from a simple classifier, or other results if discussed with the instructor\n17\n\n\nDiscussion is provided of the results and/or the potential uses or contributions of any new datasets contributed\n10\n\n\nLimitations of your approach or dataset are sufficiently discussed\n3\n\n\nEthical issues that may be raised by your system or dataset are sufficiently discussed\n3\n\n\nPotential future work is discussed\n3\n\n\nProject content total\n78\n\n\nMeets all formatting requirements. Is maximum 8 pages, not including references or group member task breakdown\n8\n\n\nWriting is clear\n9\n\n\nWriting total\n17\n\n\nGroup member had a sufficient amount of workload in the project\n13\n\n\nTask and roles assigned to this group member were completed sufficiently\n13\n\n\nIndividual contribution total\n26\n\n\nGrand total\n121"
  },
  {
    "objectID": "project.html#example-projects",
    "href": "project.html#example-projects",
    "title": "Project",
    "section": "Example projects",
    "text": "Example projects\nYour goal is to make a contribution, even a small one, to NLP research or practice. You can select from the following types of contributions, combine multiple of them, or define a different type of contribution with instructor approval. Example project ideas and projects are provided (with a bias toward computational social science and hate speech, the instructor’s research area). You are also encouraged to come up with your own ideas, too! Is there a text dataset in a field or your industry that you are familiar with that has not been analyzed? Projects can be related to students’ research, but should not be projects for other classes.\n\nNew dataset, annotations, or analysis of existing datasets\nData is at the heart of machine learning and NLP systems; it enables further modeling and encapsulates what NLP systems “know”.\n\nExample project ideas\n\n\nBuild a dataset of social media posts and ads discussing prescription drugs and natural medications for comparison, with Prof. Ryan Shi and a collaborator in Pitt Family Medicine.\nHate speech is culturally specific, yet the majority of NLP work focuses on English in North American and European contexts. A quantitative analysis of different features of datasets annotated for hate speech in multiple languages and from multiple cultural contexts would illuminate global similarities and culturally specific contexts.\nBuild a dataset of text from different personas from fiction roleplaying sites (“language cosplaying” in China). This could be useful for dialogue systems that adopt personas, or for story generation.\n\n\n\n\nNew approach or application\nThis is perhaps the most common sort of NLP research contribution, in which a new method or algorithm for approaching a task (which could be a new task) is presented. Applying an existing method in a new context or task, as might be necessary in an industry setting, would also fit within this contribution.\n\nExample project ideas\nNew tasks and applications:\n\nNew identity terms are commonly developed in online communities, some of them hateful. Develop methods to find in-group hate jargon and identity terms.\nTrace and compare the language of legislative bills introduced at state legislatures across US states. Data is provided by the instructor and a collaborator at Carnegie Mellon University.\nHate speech identification without the text: Identify the discourse contexts in which hate speech is likely to occur, without allowing classifiers to look at the exact text of the hate speech.\nFrom a set of descriptions of characters, develop a classifier to predict which ones will generate the most fanfiction. This could be a lens into online community and media norms.\nFanfiction, online writing by fans of media works, is known for celebrating queer identity but still may center the experiences of white authors and characters. Use FanfictionNLP to compare representations of characters of color to white characters in fanfiction at scale.\nAnalyze how different newspapers cover topics differently in English-language editorials from Sri Lankan newspapers. Data is provided by the instructor and a collaborator at Carnegie Mellon University.\nPredict arch support, comfort and durability of shoes from Amazon reviews, with a collaborator in Pitt Engineering.\nQuantitative analysis of hateful, white supremacist narratives usually centers on contemporary online discourse. Yet many white supremacist language and narratives has its roots before online discourse. Compare narratives, topics and themes presented in historic and contemporary white supremacist discourse with data provided by the instructor.\nEvaluate LLMs for their factuality in summarization of class reflections using a dataset provided by the instructor and Prof. Diane Litman.\nEvaluate the fairness of quality scores automatically assigned to sutdent reflections using a dataset provided by the instructor and Prof. Diane Litman.\nBuild networks of characters and predict relations among characters in fiction using this dataset.\nStancetaking, a concept from sociolinguistics, is when speakers take an evaluative position toward the concept (which are often nuanced, e.g. “No, I actually don’t like Taylor Swift’s music that much, but she’s great as a person”). Develop automated methods for identifying the “stance object”, who the speaker is evaluating, likely from Reddit data.\nAutomatically summarize movies based on their subtitles from this dataset developed by former students in the class.\nPredict “speech acts”, intentions behind utterances, based on emojis with a dataset assembled by former students in the class.\nExplore similarities and differences between language in podcasts and Reddit communities based on those podcasts using a dataset assembled by former students in the class.\nComputational analysis of Nakba narratives. See workshop and datasets.\nExamine the framing of different entities in police Facebook posts from the Plain View Project.\n\nExisting tasks to work on (some ideas are drawn from Graham Neubig’s Advanced NLP class):\n\nLexicons are often used to identify emotional language or other concepts text analysts may be interested in measuring in large corpora. Develop and evaluate approaches for expanding lexicons to specific contexts. This could be applied to expand lists of identity terms in a variety of online communities, with data supplied by the instructor.\nWASSA Shared Task on Empathy Detection and Emotion Classification and Personality Detection in Interaction\nWASSA Shared Task on Explainability for Cross-Lingual Emotion in Tweets\nSemEval Multilingual characterization and extraction of narratives from online news\nInformation retrieval from regulatory documents in a shared task and RegNLP workshop\nSemantic pleonasm detection with LLMs, on a dataset developed here at Pitt\nShared tasks in identifying AI-generated content\nAny of the SemEval 2021 tasks\nSexism identification in social networks shared task\nX-FACTR multilingual knowledge probing in QA\nGoEmotions Fine-grained Emotion Detection Dataset\nSciREX Scientific Information Extraction\nSubjective Intent Classification in Discourse\nVery Low Resource machine translation\nPredict the points at which speakers switch languages when code-switching\nSign language translation\nDevelop best approaches for training hate speech classifiers that generalize across targeted identities. Data would be provided by the instructor\nStyle transfer of offensive language into inoffensive language. See existing paper and dataset+code.\nDevelop new approaches for hate speech detection by comparing with knowledge bases or pretrained models of stereotypes from a stereotype dataset\nCross-lingual emotion detection. See existing paper.\n\n\n\n\nNew survey or position paper\nSurveys are especially needed for new, emerging research areas. All projects will require a literature review, but a survey paper would be both broader and deeper. It would summarize key approaches and key challenges and present lines for future work. Some sort of implementation is necessary for this type of contribution as well, such as applying multiple established methods to a new dataset or in a new context to show challenges that need to be addressed. Position papers argue for a certain viewpoint or shortcoming of existing approaches, e.g. arguing for the utility of techniques from a discipline outside NLP in NLP tasks.\n\nExample project ideas\n\nSurvey how NLP is used and applied in other fields. What has been our most useful contributions to scholars in the social sciences, physical sciences, or humanities? This survey would assemble papers across disciplines for mentions of NLP and summarize what is most useful, what is lacking, and what approaches from NLP could be helpful to others.\nComputational social science using NLP generally relies on data from online communities. But this is missing non-online interactions and the practices of those who are not active online. Survey datasets and approaches that use quantitative and computational techniques on recordings of offline linguistic interaction.\nA growing area of research in computational social science aims to capture the framing and portrayal of entities across large text corpora (such as in news media). Survey existing approaches and challenges."
  },
  {
    "objectID": "project.html#how-your-project-will-be-graded",
    "href": "project.html#how-your-project-will-be-graded",
    "title": "Project",
    "section": "How your project will be graded",
    "text": "How your project will be graded\nTo get an A, your group’s project should make progress toward an achievable, concrete contribution specified in your project proposal. The project does not necessarily need to be successful in the sense that it outperforms baselines or contributes to our knowledge of a phenomenon. Sometimes ideas don’t work, and that’s okay. But you need to provide evidence of progress toward that contribution. If you are building a dataset, for example, the dataset needs to be built in some form, even if it is as not as large or as useful as you may have hoped. If you are evaluating a new method for a task, you must have an implementation that tests that method against other baselines, even if it doesn’t perform as well as you would have hoped or you didn’t get to evaluate it against all the baselines you wanted to. If you are doing a survey, you must distill a sufficient number of papers into themes that comprehensively describe a research area, even if you don’t end up finding groundbreaking gaps in knowledge that must be addressed. Feel free to take on more risky ideas, but only if you know you’ll have something to show for it at the end. Teaching staff will guide you toward scoping projects that should fulfill this goal in the planning phase through the proposal."
  },
  {
    "objectID": "hw4.html",
    "href": "hw4.html",
    "title": "Homework 4: Sequence labeling",
    "section": "",
    "text": "Due 2024-11-07, 11:59pm. Instructions last updated 2024-10-24.\nIn this assignment, you will manually decode the highest-probability sequence of part-of-speech tags from a trained HMM using the Viterbi algorithm. You will also fine-tune BERT-based models for part-of-speech (POS) tagging for English and Norwegian.\nThe learning goals of this assignment are to: * Demonstrate how the Viterbi algorithm takes into account emission and transmission probabilities to find the highest-probability sequence of hidden states in an HMM * Fine-tune a transformer-based model on sequence labeling * Find and use pretrained models from Hugging Face"
  },
  {
    "objectID": "hw4.html#deliverables-for-part-1",
    "href": "hw4.html#deliverables-for-part-1",
    "title": "Homework 4: Sequence labeling",
    "section": "Deliverables for part 1",
    "text": "Deliverables for part 1\nIn your report, show your work for calculating the Viterbi tables or lattices for both example sentences. This should include backtraces and calculations for probabilities. Report the most likely tag sequences for these 2 sentences."
  },
  {
    "objectID": "hw4.html#deliverables-for-part-2",
    "href": "hw4.html#deliverables-for-part-2",
    "title": "Homework 4: Sequence labeling",
    "section": "Deliverables for part 2",
    "text": "Deliverables for part 2\nIn your report, include: \n\nThe names of the pretrained BERT-based models you chose for both English and Norwegian\nA brief discussion of any choices you made about hyperparameters in training\n(Optionally, for extra credit) A description of changes you made or different pretrained models you tried and what accuracy you obtained on the dev set. 1 point of extra credit will be given if any changes result in an improved accuracy on the dev set.\nAccuracy of the fine-tuned models on the test set for both English and Norwegian\nPOS tags predicted for the words of a sentence of your choice in both English and Norwegian\nA link to your copied and filled out Colab notebook. Please set the Colab document sharing to public viewing so that teaching staff can see it for grading."
  },
  {
    "objectID": "hw4.html#submission",
    "href": "hw4.html#submission",
    "title": "Homework 4: Sequence labeling",
    "section": "Submission",
    "text": "Submission\nPlease submit the following items on Canvas:\n\nYour report with results and answers to questions in Part 1 and Part 2, named report_{your pitt email id}_hw4.pdf. No need to include @pitt.edu, just use the email ID before that part. For example: report_mmyoder_hw4.pdf.\nA README.txt file explaining\n\nany additional resources, references, or web pages you’ve consulted\nany person with whom you’ve discussed the assignment and describe the nature of your discussions\nany generative AI tool used, and how it was used\nany unresolved issues or problems"
  },
  {
    "objectID": "hw4.html#grading",
    "href": "hw4.html#grading",
    "title": "Homework 4: Sequence labeling",
    "section": "Grading",
    "text": "Grading\nThis homework assignment is worth 56 points. See rubric on Canvas."
  },
  {
    "objectID": "hw4.html#acknowledgments",
    "href": "hw4.html#acknowledgments",
    "title": "Homework 4: Sequence labeling",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nPart 1 of this assignment is based on homework assignments by Prof. Hyeju Jang and Prof. Diane Litman. Part 2 is adapted from Jacob Eisenstein and Prof. Yulia Tsvetkov."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "",
    "text": "School of Computing and Information, University of Pittsburgh\nFall 2024"
  },
  {
    "objectID": "index.html#participation-grade",
    "href": "index.html#participation-grade",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Participation grade",
    "text": "Participation grade\nIn-class, collaborative activities are better learning experiences when students come to class and participate. To encourage participation, there is a participation grade (5% of the total course grade). The majority of that grade comes from attendance, which will be taken via TopHat on randomly selected class sessions. The rest of the grade will be assigned based on whether a student asked questions in class or otherwise (such as during office hours),or partipated in in-class activites. If you did any of this basic engagement, full credit will be awarded."
  },
  {
    "objectID": "index.html#grading-scale",
    "href": "index.html#grading-scale",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Grading scale",
    "text": "Grading scale\n\n\n\nRange\nLetter grade\n\n\n\n\n93.0 – 100%\nA\n\n\n90.0 – &lt;93.0%\nA-\n\n\n86.7 – &lt;90.0%\nB+\n\n\n83.3 – &lt;86.7%\nB\n\n\n80.0 – &lt;83.3%\nB-\n\n\n76.7 – &lt;80.0%\nC+\n\n\n73.3 – &lt;76.7%\nC\n\n\n70.0 – &lt;73.3%\nC-\n\n\n66.7 – &lt;70.0%\nD+\n\n\n63.3 – &lt;66.7%\nD\n\n\n60.0 – &lt;63.3%\nD-\n\n\n&lt; 60%\nF\n\n\n\nThe instructor reserves the right to change the grading scale depending on class performance, but only in the direction of raising grades for students. Feel free to stop by the instructor’s office hours or make an additional appointment anytime to talk about any issues you might have with your grade."
  },
  {
    "objectID": "index.html#late-work-policy",
    "href": "index.html#late-work-policy",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Late work policy",
    "text": "Late work policy\nStudents are granted 5 total late days across all homework assignments without penalty. After those five late days, you will be penalized 20% for each day that your submission is late except in extreme unforeseen circumstances. Group project work will be penalized 20% for each day late. No late work will be accepted for the final project report. Late days cannot be used for reading quizzes, as no late work is accepted for reading quizzes."
  },
  {
    "objectID": "index.html#assignment-resubmission-policy",
    "href": "index.html#assignment-resubmission-policy",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Assignment resubmission policy",
    "text": "Assignment resubmission policy\nIf you are unsatisfied with your grade on an assignment and wish to resubmit work, talk with the instructor. Resubmissions are handled case by case, but are generally accepted in cases where parts of the assignment are missing (sections of the rubric are 0). Updated or added text in resubmitted reports must be highlighted in yellow. Resubmissions are subject to an automatic 10% deduction. Only 1 resubmission per homework assignment will be accepted."
  },
  {
    "objectID": "index.html#academic-integrity-policy",
    "href": "index.html#academic-integrity-policy",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Academic integrity policy",
    "text": "Academic integrity policy\nStudents in this course will be expected to comply with the University of Pittsburgh’s Policy on Academic Integrity. Any student suspected of violating this obligation for any reason during the semester will be required to participate in the procedural process, initiated at the instructor level, as outlined in the University Guidelines on Academic Integrity To learn more about Academic Integrity, visit the Academic Integrity Guide for an overview of the topic. For hands-on practice, complete the Academic Integrity Modules.\n\nGenerative AI policy\nYou are allowed to use generative AI programs (ChatGPT, DALL-E, etc.) as a student in this course in limited circumstances. Since much of this course is about developing such tools in NLP, using currently available tools can expose you to the current capabilities and limitations of such systems.\nHowever, your ethical responsibilities as a student remain the same. You must follow the University of Pittsburgh’s Policy on Academic Integrity. Here are some principles to keep in mind that can help you determine whether or not a specific use of generative AI is acceptable in this course (for all forms of generation: writing, code, images or other forms). Please ask the instructor if you are not sure about a specific use. You will not be blamed or retaliated against for asking.\n\nUse as an aid, not for a finished product. LLMs could be used in this course to generate ideas, draft bibliographies, study guides, etc. Use for drafting entire homework or project reports is not acceptable, even if students revise this draft, since being able to communicate NLP procedures and research is a learning objective. Also keep in mind that language models have no notion of reality and will hallucinate facts and citations.\nCite its use. The University of Pittsburgh’s academic integrity policy applies to all uncited or improperly cited use of content, whether that work is created by human beings alone or in collaboration with a generative AI. If you use a generative AI tool to develop content for an assignment, you are required to cite the tool’s contribution to your work. In practice, cutting and pasting content from any source without citation is plagiarism. Likewise, paraphrasing content from a generative AI without citation is plagiarism. Similarly, using any generative AI tool without appropriate acknowledgement will be treated as plagiarism. See the APA guidelines on how to cite ChatGPT. Publicly available LLMs are very new, and so best practices in education are still being worked out. Citing your use of LLMs will also inform the instructor on how such tools are being used in education for developing better future policies.\nYou are responsible for the work you turn in. As we will discuss in this course, LLMs and other generative AI systems can and do generate biased, socially problematic language and assert unfounded claims. Ultimately the text you submit will be treated as reflecting your own work, and you are responsible for it.\n\nAdapted from faculty in the Carnegie Mellon University Heinz College of Information Systems and Public Policy, with guidance from the Carnegie Mellon University Eberly Center for Teaching Excellence."
  },
  {
    "objectID": "index.html#disability-rights",
    "href": "index.html#disability-rights",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Disability rights",
    "text": "Disability rights\nThe teaching staff of this course view disabilities as deficits not in disabled people but in the institutions and societies that are structured to disadvantage disabled people. If you have a disability (visible or invisible), please let us know as soon as possible (you don’t need to tell us the nature of the disability). You are encouraged to work with Disability Resources and Services (DRS), 140 William Pitt Union, (412) 648-7890, drsrecep@pitt.edu, (412) 228-5347 for P3 ASL users, as early as possible in the term. DRS will work with you to determine reasonable accommodations for this course. This might include lecture materials that are usable by people with visual disabilities, sign language interpretation, captioning, flexible due dates, etc.\nAdapted from policies by David Mortensen and Lori Levin at Carnegie Mellon University."
  },
  {
    "objectID": "index.html#religious-observances",
    "href": "index.html#religious-observances",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Religious Observances",
    "text": "Religious Observances\nThe observance of religious holidays (activities observed by a religious group of which a student is a member) and cultural practices are an important reflection of diversity. As your instructor, I am committed to providing equivalent educational opportunities to students of all belief systems. At the beginning of the semester, you should review the course requirements to identify foreseeable conflicts with assignments, exams, or other required attendance. Please contact me as early as possible to allow time for us to discuss and make fair and reasonable adjustments to the schedule and/or tasks."
  },
  {
    "objectID": "index.html#statement-on-scholarly-discourse",
    "href": "index.html#statement-on-scholarly-discourse",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Statement on scholarly discourse",
    "text": "Statement on scholarly discourse\nIn this course we will be discussing some complex issues on which all of us have strong feelings and, in many cases, unfounded attitudes. It is essential that we approach this endeavor with our minds open to evidence that may conflict with our presuppositions. Moreover, it is vital that we treat each other’s opinions and comments with courtesy even when they diverge and conflict with our own. We must avoid personal attacks and the use of ad hominem arguments to invalidate each other’s positions. Instead, we must develop a culture of civil argumentation, wherein all positions have the right to be defended and argued against in intellectually reasoned ways. It is this standard that everyone must accept in order to stay in this class; a standard that applies to all inquiry in the university, but whose observance is especially important in a course whose subject matter is so emotionally charged.\nAdapted from a California State University course: Race, Racism and Critical Thinking."
  },
  {
    "objectID": "index.html#student-wellness",
    "href": "index.html#student-wellness",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Student wellness",
    "text": "Student wellness\nCollege/Graduate school can be an exciting and challenging time for students. Taking time to maintain your well-being and seek appropriate support can help you achieve your goals and lead a fulfilling life. It can be helpful to remember that we all benefit from assistance and guidance at times, and there are many resources available to support your well-being while you are at Pitt. You are encouraged to visit Thrive@Pitt to learn more about well-being and the many campus resources available to help you thrive.\nIf you or anyone you know experiences overwhelming academic stress, persistent difficult feelings and/or challenging life events, you are strongly encouraged to seek support. In addition to reaching out to friends and loved ones, consider connecting with a faculty member you trust for assistance connecting to helpful resources.\nThe University Counseling Center is also here for you. You can call 412-648-7930 at any time to connect with a clinician. If you or someone you know is feeling suicidal, please call the University Counseling Center at any time at 412-648-7930. You can also contact Resolve Crisis Network at 888-796-8226."
  },
  {
    "objectID": "index.html#equity-and-inclusion",
    "href": "index.html#equity-and-inclusion",
    "title": "CS 2731 Introduction to Natural Language Processing",
    "section": "Equity and inclusion",
    "text": "Equity and inclusion\nThe University of Pittsburgh does not tolerate any form of discrimination, harassment, or retaliation based on disability, race, color, religion, national origin, ancestry, genetic information, marital status, familial status, sex, age, sexual orientation, veteran status or gender identity or other factors as stated in the University’s Title IX policy. The University is committed to taking prompt action to end a hostile environment that interferes with the University’s mission. For more information about policies, procedures, and practices, visit the Civil Rights & Title IX Compliance web page.\nI ask that everyone in the class strive to help ensure that other members of this class can learn in a supportive and respectful environment. If there are instances of the aforementioned issues, please contact the Title IX Coordinator, by calling 412-648-7860, or emailing titleixcoordinator@pitt.edu. Reports can also be filed online. You may also choose to report this to a faculty/staff member; they are required to communicate this to the University’s Office of Diversity and Inclusion. If you wish to maintain complete confidentiality, you may also contact the University Counseling Center (412-648-7930)."
  },
  {
    "objectID": "hw3.html",
    "href": "hw3.html",
    "title": "Homework 3: Character-level language modeling",
    "section": "",
    "text": "Due 2024-10-28, 11:59pm. Instructions last updated 2024-10-17.\nLanguage modeling is the task of predicting the next word in a sequence given the previous words. In this assignment, we will focus on the related problem of predicting the next character in a sequence given the previous characters. You will build character-level n-gram language models as well as train an LLM (GPT-2) to do character-level language modeling using Hugging Face. You will generate text from models you create, as well as use perplexity to measure the fit of various language models on test data related and unrelated to the training data."
  },
  {
    "objectID": "hw3.html#learning-objectives",
    "href": "hw3.html#learning-objectives",
    "title": "Homework 3: Character-level language modeling",
    "section": "Learning objectives",
    "text": "Learning objectives\nAfter completing this assignment, students will be able to:\n\nUnderstand how to compute n-gram language model probabilities using maximum likelihood estimation.\nUse n-gram and transformer-based language models to probabilistically generate texts.\nIntuitively understanding of how perplexity will estimate language model performance on unseen texts.\nGain familiarity with training LLMs using Hugging Face"
  },
  {
    "objectID": "hw3.html#extract-character-n-grams",
    "href": "hw3.html#extract-character-n-grams",
    "title": "Homework 3: Character-level language modeling",
    "section": "1.1 Extract character n-grams",
    "text": "1.1 Extract character n-grams\nFirst, fill out the ngrams(c, text) function that produces a list of all n-grams of that use c elements of context from the input text. Each n-gram should consist of a 2-element tuple (context, char), where the context is itself a c-length string comprised of the c characters preceding the current character. If c == 1, then produce bigrams, if c == 2, trigrams. The sentence should be padded with c ~ characters at the beginning (we’ve provided you with start_pad(c) for this purpose). If c == 0, all contexts should be empty strings. You may assume that c ≥ 0. You are allowed to use any resources or packages to extract the character ngrams from text, such as scikit-learn or NLTK. Here is some example output from such a function:\n&gt;&gt;&gt; ngrams(1, 'abc')\n[('~', 'a'), ('a', 'b'), ('b', 'c')]\n\n&gt;&gt;&gt; ngrams(2, 'abc')\n[('~~', 'a'), ('~a', 'b'), ('ab', 'c')]\nWe’ve also given you the function create_ngram_model(model_class, path, c, k) that will create and return an n-gram model trained on the entire file path provided."
  },
  {
    "objectID": "hw3.html#build-n-gram-language-models",
    "href": "hw3.html#build-n-gram-language-models",
    "title": "Homework 3: Character-level language modeling",
    "section": "1.2 Build n-gram language models",
    "text": "1.2 Build n-gram language models\nIn this section, you will build a simple n-gram language model that can be used to generate random text resembling a source document.\nIn the NgramModel class, write an initialization method __init__(self, c, k) which stores the context length c of the model and initializes any necessary internal variables. Then write a method get_vocab(self) that returns the vocab (this is the set of all characters used by this model).\nWrite a method update(self, text) which computes the n-grams for the input sentence and updates the internal counts. Also, write a method prob(self, context, char) which accepts a c-length string representing a context and a character, and returns the probability of that character occurring, given the preceding context. Characters that have never been seen before in a certain context would be assigned a 0 probability. If you encounter a novel context (one that has never been seen before in training data), the probability of any given character should be \\(1/V\\) where \\(V\\) is the size of the vocabulary. See Chapter 3 of the Jurafsky and Martin textbook and Equation 3.12 for calculating probabilities based on observed counts. You may not use any package to directly train/compute language model probabilities; that portion of the program should be from scratch.\n &gt;&gt;&gt; m = NgramModel(1, 0)\n &gt;&gt;&gt; m.update('abab')\n &gt;&gt;&gt; m.get_vocab()\n {'a', 'b'}\n &gt;&gt;&gt; m.update('abcd')\n &gt;&gt;&gt; m.get_vocab()\n {'a', 'b', 'c', 'd'}\n &gt;&gt;&gt; m.prob('a', 'b')\n 1.0\n &gt;&gt;&gt; m.prob('~', 'c')\n 0.0\n &gt;&gt;&gt; m.prob('b', 'c')\n 0.5\nWrite a method random_char(self, context) which returns a random character according to the probability distribution determined by the given context. Just like the prob function, in a novel context assign a probability of any given character \\(1/V\\), where \\(V\\) is the size of the vocabulary.\n\nHere is some example output. Even with setting the random seed, your output does not need to perfectly match the example output as there are multiple functions that can perform this task.\n     &gt;&gt;&gt; m = NgramModel(0, 0)\n     &gt;&gt;&gt; m.update('abab')\n     &gt;&gt;&gt; m.update('abcd')\n     &gt;&gt;&gt; random.seed(1)\n     &gt;&gt;&gt; [m.random_char('') for i in range(10)]\n     ['a', 'c', 'c', 'a', 'b', 'b', 'b', 'c', 'a', 'a']\nIn the NgramModel class, write a method random_text(self, length) which returns a string of characters chosen at random using the random_char(self, context) method. Your starting context should always be c ~ characters, and the context should be updated as characters are generated. If c == 0, your context should always be the empty string. You should continue generating characters until you’ve produced the specified number of random characters, then return the full string.\nHere is some example output. Even with setting the random seed, your output does not need to perfectly match the example output as there are multiple functions that can perform this task.\n     &gt;&gt;&gt; m = NgramModel(1, 0)\n     &gt;&gt;&gt; m.update('abab')\n     &gt;&gt;&gt; m.update('abcd')\n     &gt;&gt;&gt; random.seed(1)\n     &gt;&gt;&gt; m.random_text(10)\n     abcdbabcda"
  },
  {
    "objectID": "hw3.html#generating-shakespeare-with-character-level-n-gram-language-models",
    "href": "hw3.html#generating-shakespeare-with-character-level-n-gram-language-models",
    "title": "Homework 3: Character-level language modeling",
    "section": "1.3 Generating Shakespeare with character-level n-gram language models",
    "text": "1.3 Generating Shakespeare with character-level n-gram language models\nNow you can train a language model using the training corpus of Shakespeare. Afterward, try generating some Shakespeare with different order character n-gram models. For example, you can try using different n by running the following commands:\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 2)\n&gt;&gt;&gt; m.random_text(250)\n\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 3)\n&gt;&gt;&gt; m.random_text(250)\n\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 4)\n&gt;&gt;&gt; m.random_text(250)\n\n&gt;&gt;&gt; m = create_ngram_model(NgramModel, 'shakespeare_input.txt', 7)\n&gt;&gt;&gt; m.random_text(250)\nYou may make any additional assumptions and design decisions, but state them in your report (see below). For example, some design choices that could be made are how you want to handle uppercase and lowercase letters or how you want to handle digits. The choice made is up to you, we only require that you detail these decisions in your report and consider any implications of them in your results. There is no wrong choice here, and these decisions are typically made by NLP researchers when pre-processing data."
  },
  {
    "objectID": "hw3.html#calculate-perplexity-of-test-documents",
    "href": "hw3.html#calculate-perplexity-of-test-documents",
    "title": "Homework 3: Character-level language modeling",
    "section": "1.4 Calculate perplexity of test documents",
    "text": "1.4 Calculate perplexity of test documents\nUsing the perplexity method, calculate the perplexity of each test document. For each file in the test data (nytimes_article.txt and shakespeare_sonnets.txt), calculate the perplexity for each non-blank line and the average across all lines in the document. Do this for trigram, 4-gram and 7-gram character-level language models trained on Shakespeare plays (shakespeare_input.txt)."
  },
  {
    "objectID": "hw3.html#deliverables-for-part-1",
    "href": "hw3.html#deliverables-for-part-1",
    "title": "Homework 3: Character-level language modeling",
    "section": "Deliverables for part 1",
    "text": "Deliverables for part 1\nIn your report, include:\n\nA description of how you wrote your program, including all assumptions and design decisions\nWhat do you notice about the short passages you’ve generated from n-gram models with different n? Are they as good as 1000 monkeys working at 1000 typewriters? Are there patterns in what models generate first? Report some of your generated text and discuss.\nPerplexity values for trigram, 4-gram, and 7-gram character-level language models on each test file (New York Times article and Shakespeare sonnets).\nWhat does your perplexity indicate across different test documents? What does a comparison of different n in the n-grams in terms of perplexity tell you? Which performs best? Why do you think your models performed the way they did?"
  },
  {
    "objectID": "hw3.html#deliverables-for-part-2",
    "href": "hw3.html#deliverables-for-part-2",
    "title": "Homework 3: Character-level language modeling",
    "section": "Deliverables for part 2",
    "text": "Deliverables for part 2\nIn your report, include\n\nWhat settings you used for sampling. Did you experiment with different settings, such as the k in top-k sampling?\nA comparison of the generated output between character n-gram approaches and the GPT-2 version. Does one have more understandable words than the other? Are there any other differences you notice? Please point to specific examples."
  },
  {
    "objectID": "hw3.html#acknowledgments",
    "href": "hw3.html#acknowledgments",
    "title": "Homework 3: Character-level language modeling",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nPart 1 of this assignment is based on a homework assignment by Prof. Diane Litman and Prof. Mark Yatskar. Shakespeare data is from Andrej Karpathy."
  }
]